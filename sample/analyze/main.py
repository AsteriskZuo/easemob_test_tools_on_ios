#!/usr/bin/env python3
"""
IM æ•°æ®åˆ†æå·¥å…·

åˆ†ææ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰ JSONL æ ¼å¼çš„ IM æ•°æ®æ–‡ä»¶ï¼Œè¾“å‡ºæ•°æ®ç‰¹å¾æŠ¥å‘Šã€‚
æ¯è¡Œä¸€æ¡ JSON è®°å½•ï¼Œç”¨äºäº†è§£æ•°æ®åˆ†å¸ƒï¼Œä¸ºåç»­æŠ½æ ·æä¾›ä¾æ®ã€‚

ç”¨æ³•:
    uv run main.py <æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„>
"""

import json
import math
import os
import sys
import time
from collections import Counter
from pathlib import Path


# â”€â”€ é•¿åº¦åˆ†æ¡¶å®šä¹‰ â”€â”€
BUCKET_BOUNDARIES = [
    (200, "<200B"),
    (500, "200-500B"),
    (1024, "500B-1KB"),
    (5120, "1-5KB"),
    (float("inf"), ">5KB"),
]

# ç”¨äºè¿‘ä¼¼ç™¾åˆ†ä½æ•°çš„ç»†ç²’åº¦åˆ†æ¡¶ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½çš„å¯¹æ•°åˆ†æ¡¶ï¼‰
HIST_BOUNDARIES: list[int] = []
_v = 1
while _v <= 100_000_000:  # æœ€å¤§ ~100MB æ¯è¡Œ
    HIST_BOUNDARIES.append(_v)
    # æŒ‰ ~1.5 å€é€’å¢ï¼Œè¦†ç›–ä» 1B åˆ° 100MB
    _v = max(_v + 1, int(_v * 1.5))
HIST_BOUNDARIES.append(int(1e18))  # å“¨å…µ


class StreamingStats:
    """æµå¼ç»Ÿè®¡ï¼šä¸ä¿å­˜å…¨é‡æ•°æ®ï¼Œä½¿ç”¨åˆ†æ¡¶è¿‘ä¼¼ç™¾åˆ†ä½æ•°"""

    def __init__(self) -> None:
        self.count = 0
        self.total = 0
        self.min_val = float("inf")
        self.max_val = 0
        # å¯¹æ•°åˆ†æ¡¶ç›´æ–¹å›¾
        self.hist: list[int] = [0] * len(HIST_BOUNDARIES)

    def add(self, value: int) -> None:
        self.count += 1
        self.total += value
        if value < self.min_val:
            self.min_val = value
        if value > self.max_val:
            self.max_val = value
        # æ”¾å…¥ç›´æ–¹å›¾æ¡¶
        for i, boundary in enumerate(HIST_BOUNDARIES):
            if value < boundary:
                self.hist[i] += 1
                break

    def avg(self) -> float:
        return self.total / self.count if self.count else 0

    def percentile(self, p: float) -> float:
        """ä»ç›´æ–¹å›¾è¿‘ä¼¼è®¡ç®—ç¬¬ p ç™¾åˆ†ä½æ•° (0-100)"""
        if self.count == 0:
            return 0.0
        target = self.count * p / 100.0
        cumulative = 0
        for i, c in enumerate(self.hist):
            cumulative += c
            if cumulative >= target:
                # è¿”å›è¯¥æ¡¶çš„ä¸Šç•Œä½œä¸ºè¿‘ä¼¼å€¼
                return float(HIST_BOUNDARIES[i])
        return float(self.max_val)


def format_bytes(n: float) -> str:
    """æ ¼å¼åŒ–å­—èŠ‚æ•°ä¸ºå¯è¯»å­—ç¬¦ä¸²"""
    if n < 1024:
        return f"{n:.0f} B"
    elif n < 1024 * 1024:
        return f"{n / 1024:.1f} KB"
    else:
        return f"{n / (1024 * 1024):.2f} MB"


def get_data_files(dir_path: Path) -> list[Path]:
    """è·å–æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ•°æ®æ–‡ä»¶ï¼ˆæ’é™¤éšè—æ–‡ä»¶ï¼‰"""
    files = []
    for entry in sorted(dir_path.iterdir()):
        if entry.is_file() and not entry.name.startswith("."):
            files.append(entry)
    return files


def analyze_directory(dirpath: str) -> None:
    dir_path = Path(dirpath)
    if not dir_path.exists():
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {dirpath}")
        sys.exit(1)
    if not dir_path.is_dir():
        print(f"âŒ ä¸æ˜¯æ–‡ä»¶å¤¹: {dirpath}")
        sys.exit(1)

    data_files = get_data_files(dir_path)
    if not data_files:
        print(f"âŒ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ•°æ®æ–‡ä»¶: {dirpath}")
        sys.exit(1)

    # è®¡ç®—æ€»æ–‡ä»¶å¤§å°ï¼ˆç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰
    total_file_size = sum(f.stat().st_size for f in data_files)

    # ç»Ÿè®¡å®¹å™¨
    total_lines = 0
    parse_errors = 0
    body_type_counter: Counter[str] = Counter()
    content_type_counter: Counter[str] = Counter()
    chat_type_counter: Counter[str] = Counter()
    bucket_counts: Counter[str] = Counter()
    line_stats = StreamingStats()
    body_stats = StreamingStats()

    print(f"ğŸ“‚ åˆ†æç›®å½•: {dirpath}")
    print(f"ğŸ“„ æ•°æ®æ–‡ä»¶: {len(data_files)} ä¸ª")
    print(f"ğŸ“¦ æ€»å¤§å°: {format_bytes(total_file_size)}")
    print()

    bytes_read = 0
    last_progress_time = time.time()

    for file_idx, filepath in enumerate(data_files):
        try:
            f = open(filepath, "r", encoding="utf-8")
        except (OSError, FileNotFoundError):
            print(f"  âš ï¸  æ— æ³•æ‰“å¼€æ–‡ä»¶: {filepath.name}ï¼Œè·³è¿‡")
            continue

        with f:
            for raw_line in f:
                stripped = raw_line.strip()
                if not stripped:
                    continue

                total_lines += 1
                line_bytes = len(stripped.encode("utf-8"))
                bytes_read += line_bytes + 1  # +1 æ¢è¡Œç¬¦

                # æµå¼ç»Ÿè®¡
                line_stats.add(line_bytes)

                # è¡Œé•¿åº¦åˆ†æ¡¶
                for boundary, label in BUCKET_BOUNDARIES:
                    if line_bytes < boundary:
                        bucket_counts[label] += 1
                        break

                # è§£æ JSON
                try:
                    record = json.loads(stripped)
                except json.JSONDecodeError:
                    parse_errors += 1
                    continue

                # content_type
                ct = record.get("content_type", "unknown")
                content_type_counter[ct] += 1

                # chat_type
                chat_t = record.get("chat_type", "unknown")
                chat_type_counter[chat_t] += 1

                # payload.bodies[0]
                payload = record.get("payload", {})
                bodies = payload.get("bodies", [])
                if bodies:
                    body = bodies[0]
                    body_type = body.get("type", "unknown")
                    body_type_counter[body_type] += 1
                    body_json = json.dumps(body, ensure_ascii=False)
                    body_bytes = len(body_json.encode("utf-8"))
                    body_stats.add(body_bytes)
                else:
                    body_type_counter["(empty)"] += 1
                    body_stats.add(0)

                # è¿›åº¦æ˜¾ç¤ºï¼ˆæ¯ç§’æœ€å¤šæ›´æ–°ä¸€æ¬¡ï¼‰
                now = time.time()
                if now - last_progress_time >= 1.0:
                    pct = bytes_read / total_file_size * 100 if total_file_size else 0
                    print(
                        f"\r  â³ è¿›åº¦: {pct:.1f}% | "
                        f"æ–‡ä»¶ {file_idx + 1}/{len(data_files)} | "
                        f"å·²å¤„ç† {total_lines:,} è¡Œ",
                        end="",
                        flush=True,
                    )
                    last_progress_time = now

    # æ¸…é™¤è¿›åº¦è¡Œ
    print(f"\r{' ' * 80}\r", end="")

    # â”€â”€ è¾“å‡ºæŠ¥å‘Š â”€â”€
    print("=" * 60)
    print("  ğŸ“Š æ•°æ®åˆ†ææŠ¥å‘Š")
    print("=" * 60)

    # åŸºæœ¬ä¿¡æ¯
    print(f"\n{'â”€â”€ åŸºæœ¬ä¿¡æ¯ â”€â”€':â”€^56}")
    print(f"  æ•°æ®æ–‡ä»¶æ•°:         {len(data_files)}")
    print(f"  æ€»è®°å½•æ•° (æœ‰æ•ˆè¡Œ):  {total_lines:,}")
    print(f"  è§£æå¤±è´¥:           {parse_errors}")
    print(f"  æ€»æ–‡ä»¶å¤§å°:         {format_bytes(total_file_size)}")
    if total_lines > 0:
        avg_line = total_file_size / total_lines
        print(f"  å¹³å‡è¡Œå¤§å°:         {format_bytes(avg_line)}")

    # å„æ–‡ä»¶ä¿¡æ¯
    print(f"\n{'â”€â”€ æ–‡ä»¶åˆ—è¡¨ â”€â”€':â”€^56}")
    for fp in data_files:
        try:
            size_str = format_bytes(fp.stat().st_size)
        except (OSError, FileNotFoundError):
            size_str = "(ä¸å¯ç”¨)"
        print(f"  {fp.name:30s} {size_str:>10s}")

    # body type åˆ†å¸ƒ
    print(f"\n{'â”€â”€ bodies[0].type åˆ†å¸ƒ â”€â”€':â”€^50}")
    print(f"  {'ç±»å‹':<20} {'æ•°é‡':>8} {'å æ¯”':>10}")
    print(f"  {'â”€' * 20} {'â”€' * 8} {'â”€' * 10}")
    for t, count in body_type_counter.most_common():
        pct = count / total_lines * 100 if total_lines else 0
        print(f"  {t:<20} {count:>8,} {pct:>9.1f}%")

    # content_type åˆ†å¸ƒ
    print(f"\n{'â”€â”€ content_type åˆ†å¸ƒ â”€â”€':â”€^50}")
    print(f"  {'ç±»å‹':<35} {'æ•°é‡':>8} {'å æ¯”':>10}")
    print(f"  {'â”€' * 35} {'â”€' * 8} {'â”€' * 10}")
    for t, count in content_type_counter.most_common():
        pct = count / total_lines * 100 if total_lines else 0
        print(f"  {t:<35} {count:>8,} {pct:>9.1f}%")

    # chat_type åˆ†å¸ƒ
    print(f"\n{'â”€â”€ chat_type åˆ†å¸ƒ â”€â”€':â”€^50}")
    print(f"  {'ç±»å‹':<20} {'æ•°é‡':>8} {'å æ¯”':>10}")
    print(f"  {'â”€' * 20} {'â”€' * 8} {'â”€' * 10}")
    for t, count in chat_type_counter.most_common():
        pct = count / total_lines * 100 if total_lines else 0
        print(f"  {t:<20} {count:>8,} {pct:>9.1f}%")

    # é•¿åº¦ç»Ÿè®¡
    if line_stats.count > 0:
        print(f"\n{'â”€â”€ æ•´è¡Œé•¿åº¦ç»Ÿè®¡ (bytes) â”€â”€':â”€^48}")
        print(f"  Min:   {format_bytes(line_stats.min_val)}")
        print(f"  Max:   {format_bytes(line_stats.max_val)}")
        print(f"  Avg:   {format_bytes(line_stats.avg())}")
        print(f"  P50:   {format_bytes(line_stats.percentile(50))}")
        print(f"  P90:   {format_bytes(line_stats.percentile(90))}")
        print(f"  P99:   {format_bytes(line_stats.percentile(99))}")

        print(f"\n{'â”€â”€ bodies[0] é•¿åº¦ç»Ÿè®¡ (bytes) â”€â”€':â”€^44}")
        print(f"  Min:   {format_bytes(body_stats.min_val)}")
        print(f"  Max:   {format_bytes(body_stats.max_val)}")
        print(f"  Avg:   {format_bytes(body_stats.avg())}")
        print(f"  P50:   {format_bytes(body_stats.percentile(50))}")
        print(f"  P90:   {format_bytes(body_stats.percentile(90))}")
        print(f"  P99:   {format_bytes(body_stats.percentile(99))}")

    # é•¿åº¦åˆ†æ¡¶
    print(f"\n{'â”€â”€ è¡Œé•¿åº¦åˆ†æ¡¶ â”€â”€':â”€^52}")
    print(f"  {'æ¡¶':<12} {'æ•°é‡':>8} {'å æ¯”':>10} {'æŸ±çŠ¶å›¾'}")
    print(f"  {'â”€' * 12} {'â”€' * 8} {'â”€' * 10} {'â”€' * 20}")
    max_count = max(bucket_counts.values()) if bucket_counts else 1
    for _, label in BUCKET_BOUNDARIES:
        count = bucket_counts.get(label, 0)
        pct = count / total_lines * 100 if total_lines else 0
        bar_len = int(count / max_count * 20) if max_count else 0
        bar = "â–ˆ" * bar_len
        print(f"  {label:<12} {count:>8,} {pct:>9.1f}% {bar}")

    print()
    print("=" * 60)
    print("  åˆ†æå®Œæˆ âœ…")
    print("=" * 60)


def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: uv run main.py <æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„>")
        print("ç¤ºä¾‹: uv run main.py ../../sdkTest/tmp")
        sys.exit(1)

    analyze_directory(sys.argv[1])


if __name__ == "__main__":
    main()
